{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed2fdf2",
   "metadata": {},
   "source": [
    "# ðŸ§  Enhanced EDA Pipeline for Selecting Universal Learning Metrics\n",
    "\n",
    "This notebook presents a **step-by-step exploration and dimensionality reduction process** aimed at identifying the **3â€“5 most representative learning metrics** that can be monitored to provide **personalized, feedback-driven learning optimization**.\n",
    "\n",
    "### ðŸŽ¯ Project Goal:\n",
    "We aim to find the **minimum subset of behavioral and cognitive metrics** that:\n",
    "- Vary meaningfully between different types of learners\n",
    "- Represent key dimensions of how people engage, think, and grow\n",
    "- Can be **monitored over time** to personalize feedback and improve outcomes\n",
    "\n",
    "By understanding these metrics, we can go beyond model optimization and begin to **optimize the learner**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d22a2d",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preview the Learning Data\n",
    "\n",
    "The dataset contains 22 numeric learning metrics across 4422 examples. These include:\n",
    "- Engagement\n",
    "- Error behavior\n",
    "- Confidence and emotion\n",
    "- Self-reflection\n",
    "- Knowledge transfer\n",
    "\n",
    "Each row represents an observation of a learning process, and we aim to extract the **essence of learning** from these raw signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a75af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('merged_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6ee8",
   "metadata": {},
   "source": [
    "## Step 2: Preprocessing and Standardization\n",
    "\n",
    "Before applying any statistical or unsupervised learning methods, we remove identifiers and **standardize all features** to ensure equal weighting across metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_numeric = df.drop(columns=['source_model'])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fae519",
   "metadata": {},
   "source": [
    "## Step 3: Descriptive Statistics and Correlation Structure\n",
    "\n",
    "We visualize correlation to:\n",
    "- Spot highly correlated or redundant features\n",
    "- Begin to form intuition around **clusters of meaning** â€” e.g. emotion, confidence, reflection, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8289b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(pd.DataFrame(X_scaled, columns=df_numeric.columns).corr(), cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix of Learning Metrics\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aaccf2",
   "metadata": {},
   "source": [
    "## Step 4: Principal Component Analysis (PCA)\n",
    "\n",
    "### Why PCA?\n",
    "PCA helps reduce dimensionality while retaining most of the variance in the data. It uncovers **latent structures** that may not be obvious â€” like composite traits of a learner (e.g. \"curious and confident\").\n",
    "\n",
    "We use PCA to:\n",
    "- Reduce the number of noisy variables\n",
    "- Prepare data for clustering\n",
    "- Begin to understand the **main drivers** of learning behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA()\n",
    "pca_components = pca.fit_transform(X_scaled)\n",
    "explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(1, len(explained_variance)+1), explained_variance, marker='o')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--')\n",
    "plt.title('Cumulative Variance Explained by PCA')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91393d50",
   "metadata": {},
   "source": [
    "> From the graph, we retain **6 components** which together explain over **90%** of the variance in learner metrics. This balance ensures we capture sufficient complexity while keeping the model interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e40ab9",
   "metadata": {},
   "source": [
    "## Step 5: Nonlinear Embedding with t-SNE and UMAP\n",
    "\n",
    "Linear techniques like PCA are powerful but might miss complex patterns. We use:\n",
    "- **t-SNE** (t-distributed stochastic neighbor embedding): great for visualizing cluster structure\n",
    "- **UMAP** (Uniform Manifold Approximation): better preserves local and global relationships\n",
    "\n",
    "These allow us to **visualize the structure of learners** in a low-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30453399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.5, s=10)\n",
    "plt.title(\"t-SNE Projection of Learner Metrics\")\n",
    "plt.xlabel(\"TSNE-1\")\n",
    "plt.ylabel(\"TSNE-2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1], alpha=0.5, s=10)\n",
    "plt.title(\"UMAP Projection of Learner Metrics\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb49ce65",
   "metadata": {},
   "source": [
    "## Step 6: Clustering and Finding Learner Types\n",
    "\n",
    "To discover distinct **learning phenotypes**, I use K-Means clustering in PCA space and silhouette scores to pick the best number of clusters.\n",
    "\n",
    "This helps segment learners into types like:\n",
    "- *Exploratory learners*\n",
    "- *Efficient executors*\n",
    "- *Reflective revisers*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956dc12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "ks = range(2, 11)\n",
    "scores = []\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(pca_components[:, :6])\n",
    "    score = silhouette_score(pca_components[:, :6], labels)\n",
    "    scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ks, scores, marker='o')\n",
    "plt.title(\"Silhouette Scores for Different K\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04adde2",
   "metadata": {},
   "source": [
    "> We choose **K = 3**, as it gives a good tradeoff between interpretability and separation quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63763a76",
   "metadata": {},
   "source": [
    "## Step 7: Hierarchical Clustering (Bonus)\n",
    "\n",
    "For completeness, I also tried hierarchical clustering, which builds a tree of learners without requiring us to fix K. This reveals **subtypes** and relationships between learners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "Z = linkage(X_scaled, method='ward')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z, truncate_mode='level', p=5)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram (truncated)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a8a41",
   "metadata": {},
   "source": [
    "## Step 8: Feature Selection â€” Which Metrics Really Matter?\n",
    "\n",
    "To identify the **3â€“5 metrics** that best distinguish learning types, I analyze:\n",
    "- **Variance across clusters** (via ANOVA F-score)\n",
    "- **Interpretability** and relevance to learning optimization\n",
    "\n",
    "These will form our **universal monitoring metrics**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "kmeans_final = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(pca_components[:, :6])\n",
    "\n",
    "f_values, p_values = f_classif(X_scaled, cluster_labels)\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': df_numeric.columns,\n",
    "    'F_value': f_values,\n",
    "    'p_value': p_values\n",
    "}).sort_values(by='F_value', ascending=False)\n",
    "\n",
    "feature_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b2aed",
   "metadata": {},
   "source": [
    "## Step 9: Interpretation and Final Metric Selection\n",
    "\n",
    "The top 5 metrics are selected based on:\n",
    "1. **High variance across clusters**\n",
    "2. **Cognitive or behavioral interpretability**\n",
    "3. **Relevance to learning feedback and support**\n",
    "\n",
    "| Metric                        | Why It's Selected |\n",
    "|------------------------------|--------------------|\n",
    "| **Vocabulary Expansion Count** | Tracks growing knowledge and conceptual use |\n",
    "| **Curiosity Level**            | Key driver of self-directed learning |\n",
    "| **Confidence Level**           | Reflects readiness and internal self-estimation |\n",
    "| **Output Compression Ratio**   | Measures clarity and abstraction |\n",
    "| **Emotional Consistency Score**| Emotional engagement and regulation |\n",
    "\n",
    "These metrics represent a **minimal set** that I can monitor continuously to understand:\n",
    "- Learner readiness\n",
    "- Motivation\n",
    "- Growth curve\n",
    "- Burnout risk\n",
    "- Feedback responsiveness\n",
    "\n",
    "With just these, I can drive personalized, human-centric learning optimization.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
